<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Future of AI - Paul Cherian</title>
    <link rel="stylesheet" href="../css/styles.css">
</head>
<body>
    <header>
        <h2>Paul Cherian</h2>
        <a href="../index.html">Home</a>
        <a href="../about.html">About</a>
        <a href="../writing.html">Writing</a>
        <a href="../courses.html">Courses</a>
        <a href="../projects.html">Projects</a>
    </header>
    <main>
        <h1>Emerging AI Technologies</h1>
        <div class="essay-date">August 2, 2024</div>
        
        <article class="essay-content">
            <p>This essay was adapted from a memo I wrote for Vine Ventures in August of 2024. It outlines some key AI technologies that I think will be revolutionary in the near future.</p>
            
            <h2>A preface on AGI</h2>
            <p>
            Artificial General Intelligence — defined as AI with human-level cognition capabilities — would be the largest technological
            advance in the history of civilization. There are many perspectives on when (or if) it will be achieved, by whom, and how.
            OpenAI leadership claims that it will arrive within the next five to fifteen years and Anthropic CEO Dario Amodei forecasts an
            even more aggressive timeline of two to three. Others, such as Yann LeCun of Meta and François Chollet of Google are skeptical
            about our current path and forecast longer timelines for its arrival. In both views, much of this is speculation and the simple
            truth is that no one theory can be validated to confidently predict the next, five, fifty or five-hundred years of AI advancement.
            With that being said, there is value in understanding current perspectives, and on a fundamental level, there are two camps of
            AGI predictions in today's environment:</p>

            <p>Camp #1: Scaling patterns and emergent properties will continue to hold and the data wall will be overcome with synthetic data.</p>

            <p>This perspective extrapolates from current experimental trends of training Large Language Models. It assumes that scaling patterns are actually scaling laws and that at some point training with more compute will eventually lead to a model of AGI sophistication. On this road, this group expects more human (or superhuman) abilities to emerge with model scale and for AGI to be achieved in the near future (&lt;10 years). While these two properties of models have been true in recent years, they are fundamentally experimental observations without sufficient theory to explain the phenomena they rely on - additionally, their performance has only scaled linearly with exponential increases in compute (see diagram above). While their precedent might be strong enough to warrant training the next larger model, there is still no guarantee scaling laws will continue in perpetuity. Thus, I believe this is a weak argument to hold.</p>

            <p>Camp #2: Our current architectures and model scaling/size can be useful technology, but their progress is not a path toward true AGI.</p>

            <p>This is a view that rejects the assumptions Camp #1 makes. Specifically, they do not grant that scaling patterns will hold, and even if they should, they doubt the quantity and quality of new data that will even be available to do so. This group does not see transformer architectures as a road to AGI and believes a fundamental architecture switch is necessary. Thus, they do not forecast a near-term timeline for arriving at AGI. I find this a stronger perspective for two main reasons: 1. LLMs <a href="#">have not</a> demonstrated critical reasoning at a general level. 2. The data wall may be a significant barrier for feasibly scaling models in the first place.</p>

            <p>AGI could usher humans into a new era of prosperity. This is the consensus that <a href="#">many</a> who are reaching for AGI proclaim. They recall previous jumps in humanity's technological progress — agriculture, the automobile, the computer — and observe that while at the precipice of each technological marvel, there was always <a href="#">technological anxiety</a> about losing humanity with an increased abstraction of work. They argue that this anxiety has been disproven and that in each case, technological advancements actually provided ample prosperity and increased humans' quality of life. To the question: <i>Is this time different?</i> Proponents of AGI answer <i>No</i> and predict a society of abundance, prosperity, and happiness.</p>

            <p>However, this perspective also disregards some of the damming implications for the world if AGI is achieved. Many reasons are that if the marginal cost of human intelligence is nearly 0 with AGI, there will be disastrous outcomes from the malicious deployment of it - which would be virtually impossible to stop. These are salient points. Why wouldn't nations catalyze the production of weapons, massive cybersecurity attacks, and election-interference techniques with AGI? AGI is an accelerant for any process that would typically be constrained by human labor - <i>both good and bad</i>. If AGI were achieved, humans would likely become a multi-planetary species soon after. Science and technology would quickly evolve using super-powered clusters of artificial researchers. Capitalist markets would effectively crumble with commoditized intelligence, forcing governments to switch to UBI or other state-run means of income redistribution. The world might not be resource-poor, but it certainly would be upheaved with AGI.</p>

            <p>In a world with AGI, all information jobs would effectively be up for automation. In fact, if AGI is possible, it is also much more likely that ASI —meaning Artificial Super Intelligence with capabilities far exceeding humans— is achievable soon after, with the full force of replicable AGIs working to create it. These above ramifications of AGI become even more severe when ASI is considered. While AGI might be a commoditized technology in a future world, ASI is more likely to be one of the most closely guarded state weapons if it were achieved. As a scalable way to accelerate scientific progress and really accomplish any task in a superior manner, any actor who came into control of ASI would immediately be equipped with the most advanced weapon humanity has ever seen. With as much compute as the world could produce, these ASI would be free to learn every domain, read every research paper, and every blog post, and learn from all parallel instances of its deployment. It would also be difficult or nearly impossible to understand its actions. Just like how <a href="#">Move 37</a> perplexed the leading Go players when DeepMind's AlphaGo played it (and eventually led to its victory), ASI's intelligence and motives, and therefore its actions, may be incomprehensible to humans as well.</p>

            <p>In the current regulatory framework, it is likely that democracies will try to continue to regulate AI's development and AGI deployment on arrival. Already, the EU has put guardrails on the harmful use and production of AI which they outline in their <a href="#">EU AI Act</a>. Tech leaders like Sundar Pichai are also calling for <a href="#">important, but fair regulation</a> of the technology. Even US lawmakers are concerned with the rapid progression of the technology, spurring <a href="#">hundreds of acts</a> of AI-related legislation being put up for debate. However some leaders, particularly in the <a href="#">Silicon Valley AI</a> community and <a href="#">venture capital</a> ecosystem, have been pushing back on these broad-stroke regulatory proposals. While this regulation is largely supported by governments, in many ways I think it is doomed to fail. Compared to other dangerous technologies, AI is much 'easier' to progress, and without a strict ban on computing, it's unlikely that people anywhere would willingly stop accelerating.</p>

            <p>Regardless of which camp on AGI you lean toward or its possible future implications and regulation, there is no denying that the current technologies at our disposal are creating a revolution in today's age. Future progress of AI models will only continue to democratize intelligence, affecting every part of our world. While under the current model progress paradigm believed by Camp #1, companies like OpenAI, Anthropic, Google, and Meta may seem best positioned to achieve AGI. But if reality skews toward Camp #2's vision, these incumbents are not necessarily guaranteed to be the first to AGI. Too much focus on the current paradigm of simply training bigger and better models as a road toward AGI may actually set these incumbents back compared to startup competitors. This progress is new and as such there is possibility and speculation sewn within every perspective. As investors in the frontier, however, it is Vine's job to ask these questions about what future paradigms could look like and what tools, technologies, and problems will be wrapped up within them. As such, I have compiled five exciting research fields that could be the foundations for major paradigm shifts in the future. As an investing firm, these areas will spawn new opportunities for domain-specific technical innovation and platforms for supporting that innovation.</p>

            <h2>Federated Learning</h2>
            
            <h3>Overview</h3>
            <p>Federated Learning (FL) is a privacy-preserving machine learning paradigm. It allows organizations to use sensitive data that, in traditional machine learning, would be siloed on individual users’ devices. FL is secure because it trains models without needing to move, aggregate, and store data on a centralized server. It does so by only sharing gradients (model updates) over a network and averaging updates over a large number of devices.</p>
            
            <h3>Why does it matter?</h3>
            <p>FL is an important unlock for massive amounts of formerly unusable data. With FL, edge devices become sources of usable data, but also model training. For example, Apple and Google almost exclusively use FL to train their <a href="#">speech recognition</a> and <a href="#">autocomplete models</a> since they can take advantage of their users’ private data. In <a href="#">healthcare</a>, it is vital to training models since in many cases, sensitive patient data can not legally be centralized. FL has a clear use case and allows for more powerful models to be trained compliantly.</p>
            
            <h3>What's next?</h3>
            <p>As edge devices become more powerful and plentiful with each new generation of phones and laptops, FL may evolve to become a new way of distributed computing, not just a privacy-preserving paradigm. In the current FL paradigm, the training and inference are run on-device - after users’ data makes a forward pass through the model, backpropagation is used to compute the gradient which is then sent over the network to be averaged with other devices’ computed gradients. Crucially, within this entire process, the intensive compute processes happen on edge devices. In the future, as these devices become more powerful, I and others expect the FL paradigm to become the dominant training method.</p>
            
            <h3>Key Research Papers</h3>
            <ul>
                <li><a href="#">Communication-Efficient Learning of Deep Networks from Decentralized Data</a>(McMahan et al., February 2017)</li>
                <li><a href="#">Deep Leakage from Gradients</a>(Zhu et al., June 2019)</li>
                <li><a href="#">Generative Models for Effective ML on Private, Decentralized Datasets</a></li>
                <li><a href="#">Flower: A Friendly Federated Learning Research Framework</a>(Beutel et al., July 2020)</li>
            </ul>

            <h2>Scaling Laws and Emergent Abilities</h2>

            <h3>Overview</h3>
            <p>Scaling laws articulate the most general principle pushing AI today: ‘scaling compute (data x model size) predictably decreases loss.’ In the last four years, researchers have also discovered emergent abilities that appear when scaling LLMs. For example, take the task of 3-digit addition. As recorded in a paper from Anthropic, until scaling models past a parameter threshold of 10^10, models recorded roughly 0% performance when adding triple digits. After crossing it however, they immediately spike to an accuracy of over 80%. These abilities are called emergent since they are non-existent in smaller models up until some compute threshold, at which point scaling such models increases performance linearly. Though there are some rebuttals against the idea that emergent abilities are a product of scaling models (and instead due to metrics), largely the evidence in favor of emergence is overwhelming and forms the current paradigm for AI in research and industry.</p>
            
            <h3>Why Does it Matter></h3>
            <p>Scaling laws and emergent abilities have been the core motivators behind the LLM boom of the last three years. In short, massive model providers have taken (so far winning) bets that with an increase in training parameters and data for a model, they will find better performance on current tasks and gain emergent abilities on new ones. Largely, this has been correct as of my writing this in July 2024. These two phenomena, however, are only experimentally validated — that is to say, while we observe scaling laws and emergence to be true, there is no proven rationale for either. Thus, while they could hold in the future, there is no guarantee. As a venture capital firm investing in the AI revolution, tracking whether these fundamental laws of LLM progress hold or break will be crucial to deciding whether to underwrite businesses that are riding the AI wave - if core AI models can never reach a performance level that is suitable for commercial use-cases, there is a huge risk to any business building for the ecosystem.</p>
            
            <h3>What's Next?</h3>
            <p>The incumbents (OpenAI, Anthropic, Google, Meta, etc.) who believe scaling laws will hold will continue to invest in larger models and the AI infrastructure around it. These companies will require more investment, more compute, and more energy. If scaling laws hold, expect NVIDIA to become an even better investment. If scaling laws show signs of stopping, investment will turn to new architectures to progress toward AGI. Companies like Cartesia, Symbolica, and more alternative architecture companies will emerge, hoping to find a new paradigm.</p>
            <p>In my personal opinion, until there is an accepted explanation, rather than just experimental validation, behind scaling patterns and emergent properties of LLMs, it is difficult to assume that these trends will continue to hold. Furthermore, there are real questions about whether we will have the ability to even continue testing parts of the scaling hypothesis as current models are approaching the upper limit of human-generated training data and continue to be extremely time/compute intensive to train.</p>

            <h3>Key Research Papers</h3>
            <ul>
                <li><a href="#">The Bitter Lesson</a> (Sutton, March 2019)</li>
                <li><a href="#">Scaling Laws for Neural Language Models</a> (Kaplan et al., January 2020)</li>
                <li><a href="#">Emergent Abilities of Large Language Models</a> (Wei et al., June 2022)</li>
                <li><a href="#">Are Emergent Abilities of Large Language Models a Mirage?</a> (Schaeffer et al., April 2023)</li>
            </ul>
            
            <h2>Attention, Context Length, and SSMs</h2>

            <h3>Overview</h3>
            <p>The attention mechanism is the crux of the transformer architecture. It is how standalone word embeddings are numerically updated to be imbued with the context of their sentence. However, the cost of performing this update grows quadratically with the context size (since each token is compared to tokens before it in a sentence)—thus finding ways to circumnavigate this bottleneck has been an important area of research in recent years. <a href="#">State Space Models</a> are a promising architecture category that scales linearly in complexity, allowing for applications that require much higher throughput of data. SSMs solve a set of linear state-space equations to predict an output sequence and while SSMs are <a href="#">not a new modeling approach</a>, the contribution of <a href="#">Mamba</a>, which specialized it for language modeling, has quickly gained popularity as a transformer competitor.</p>

            <h3>Why does it matter?</h3>
            <p> Any shift from the massive incumbent transformer architecture is important to be aware of. In particular, with State Space Models, new applications that can take advantage of a larger context length capacity can be built. For developers who are clued into this research, these architectures are new opportunities for better product performance. Already, there are companies like <a href="#">Cartesia.ai</a> that have spun out of this research into sub-quadratic architectures, and long-context foundation models like <a href="#">Evo</a> for specific verticals have been released using other types of attention workarounds. Transformers still have an edge for shorter sequences that demand higher accuracy, but the tradeoff that SSMs allow for can be extremely useful when long context is a priority.</p>
            <h3>What's next?</h3>
            <p>I expect that most of the energy around SSMs will go into finding ways to match transformer performance on a variety of tasks, and continue to develop new models that are best suited for long-context applications. Because context length is such an important bottleneck for a variety of operations, research will continue to look for attention workarounds within the transformer architecture and new sub-quadratic methods in other architectures.</p>

            <h3>Key Research Papers</h3>
            <ul>
                <li><a href="#">Attention is All You Need</a> (Vaswani et al., June 2017)
                </li>
                <li><a href="#">Efficiently Modeling Long Sequences with Structured State Spaces</a> (Gu et al., October 2021)
                </li>
                <li><a href="#">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</a> (Gu et al., December 2023)
                </li>
            </ul>

            <h2>AI and Robotics</h2>

        <h3>Overview</h3>
        <p>
        At a core level, classical robotic learning has always been stifled by a series of ‘death by a thousand cuts’ problems, where there has been a success in training models for very specific applications, but the breadth and variety of problems necessary to make an actually useful robotic system has hindered their progress. Foundation models, trained on a vast magnitude and distribution of data, have been excellent at tackling these breadth-first tasks and being seen as a <a href="#">new technology</a> for robotics to take advantage of. Historically, robotics has been a software, not a hardware-constrained task, but the foundation models' unique ability to tackle general tasks learned from massive datasets makes them an exciting solution as a catalyst for robotics progress.
        </p>

        <h3>Why does it matter?</h3>
        <p>
        These new research techniques have the potential to create general-purpose robots with the ability to interact with their environment. While foundation models have struggled with perfection on deeper, detail-oriented tasks, their ability to solve a breadth of problems still has the potential to revolutionize robotics in a number of markets. For instance, even with simple advances in <a href="#">locomotion</a>, jobs in the massive <a href="#">service industry</a> are ripe for adoption. These can then take advantage of existing LLM Q/A technology embedded in physical forms. Additionally, a large portion of <a href="#">manufacturing</a> still relies heavily on human manipulation and dexterity for various assembly line tasks. Advances in AI are both an enabler for general-purpose robots to exist with efficacy and a feature that they can implement to spur their adoption in the real world.
        </p>

        <h3>What's next?</h3>
        <p>
        Just as foundation models have been developed for vision, audio, and text, researchers are spending time working to develop foundation models for robotics—both from <a href="#">industry</a> and <a href="#">research</a>. While these datasets and models specific to robot manipulation are growing, there is also exciting research on how to use existing foundation models for robotic control—see the <a href="#">Vision-Language-Action</a> model architecture. If these breakthroughs come to fruition and are able to solve the ‘death by a thousand cuts’ problems that robotics have faced in the past, expect to see many more robots applied to commercial tasks.
        </p>

        <h3>Key Research Papers</h3>
        <ul>
            <li><a href="#">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a> (Finn et al., March 2017)</li>
            <li><a href="#">Sim-to-Real: Learning Agile Locomotion For Quadruped Robots</a> (Tan et al., April 2018)</li>
            <li><a href="#">RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control</a> (Brohan et al., July 2023)</li>
            <li><a href="#">Open X-Embodiment: Robotic Learning Datasets and RT-X Models</a> (Open X-Embodiment Collaboration, October 2023)</li>
        </ul>

        <h2>Agents and Agentic Workflows</h2>

        <h3>Overview</h3>
        <p>
        LLMs have proven themselves to be a strong tool in chatbots, and question-answer prompt scenarios. Agents take advantage of these querying, next-word prediction models and allow them to be used to influence, react, and use tools in their environments. Research and development of agents is what will turn interesting conversational chatbots into remarkable human replacements with a huge magnitude of change.
        </p>

        <h3>Why does it matter?</h3>
        <p>
        Using agentic workflows can improve the performance of models and allow LLMs to have the ability to interact with their environment. This is a massive value-unlock for AI models and will help transition improvements in performance from the research side to having real-world impact. Many researchers also believe that compound AI agentic systems (which in my opinion also serve as a natural corollary from the structure of the human brain) will be the path to the best performance in the future.
        </p>

        <h3>What's next?</h3>
        <p>
        Expect workflows will continue to improve both in quality and in the scope and complexity of tasks they will be deployed for. Model providers are continuing to improve their LLMs and this will translate to direct improvements in agentic behaviors—especially when smaller models are improved, agentic workflows will massively benefit from a lower inference cost (queries for agents usually translate into numerous repeated inference calls which can become extremely expensive). Research into multi-agent systems is also a promising direction for the subfield, as well as <a href="#">multimodal agents</a> that can read from and interact with text, vision, and audio. Combined with <a href="#">improving benchmarks</a> to evaluate these systems, expect the frontier of agentic systems to grow as pushed by research and industry.
        </p>

        <h3>Key Research Papers</h3>
        <ul>
            <li><a href="#">Toolformer: Language Models Can Teach Themselves to Use Tools</a> (Schick et al., February 2023)</li>
            <li><a href="#">Reflexion: Language Agents with Verbal Reinforcement Learning</a> (Shinn et al., March 2023)</li>
            <li><a href="#">SWE-bench: Can Language Models Resolve Real-world Github Issues?</a> (Jimenez et al., October 2023)</li>
        </ul>





        </article>
    </main>
</body>
</html> 