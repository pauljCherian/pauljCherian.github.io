<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Scaling and Research- Paul Cherian</title>
    <link rel="stylesheet" href="../css/styles.css">
</head>
<body>
    <header>
        <h2>Paul Cherian</h2>
        <a href="../index.html">Home</a>
        <a href="../about.html">About</a>
        <a href="../writing.html">Writing</a>
        <a href="../courses.html">Courses</a>
        <a href="../projects.html">Projects</a>
    </header>
    <main>
        <h1>Scaling Phenomena and a direction for AI Research</h1>
        <div class="essay-date">November 28, 2023</div>
        
        <article class="essay-content">
            
            <p>As someone fascinated by the frontier of AI technologies and its incredible expansion in the last 3 years, it's unequivocally incredible what capabilities have been unlocked by an idea as simple as scaling. Ilya Sutskever, and more broadly OpenAI's bet that simply adding more compute and more data into their models has clearly paid off in a massive way. ChatGPT and it's competitors in Sonnet, Gemini, and Llama, are truly revolutionary in what they empower in builders, learners, and thinkers.</p>
            <p>However, I think "Scaling Law" is too confident a term for something that we don't fundamentally understand. Yes, we have observed and can maybe even find some intuition for the idea that more data and more compute engenders better performance. But ultimately these Scaling Laws are mostly experimental phenomena we observe, but can not deeply explain. </p>
            <p>At this moment, the hyperscalers have been forced into a tragedy of the commons with their investment into AI, predicated on the idea that this scaling phenomena will hold true. Simply put, the promise of AI and the advantages of nearing AGI are too great for any company to not enter into the AI arena. Because these companies all have a "cash-cow" business to fund it, it's their prerogative to continue pushing their risk parameters to invest in AI. </p>
            <p>On the research side, however, I am more interested in the supposition that the Scaling <i>Hypothesis</i> is not true, a growing sentiment in the AI community. It very well might be that scaling is the least risky way to ensure better performance, but as a prospective researcher, I find it a much more interesting question to discover how we can get better performance with less, rather than with more. In the near future, I hope to explore this field of model compression and distillation. Intuitively, I see compression as a mathematical analog for comprehension â€” though I might need a better grasp on information theory to validate this idea. </p>
            
            <!-- <p>You can also include <a href="#">links</a> and other HTML elements as needed within your essays.</p> -->
        </article>
    </main>
</body>
</html> 